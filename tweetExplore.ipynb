{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- leave running for 10+ hours to get ~50,000 tweets\n",
    "- use keywords for scraping filter \n",
    "- try to figure something out relevant to content\n",
    "- given sentiment analysis looks innaccurate - does it still correlate with favorable/unfavorable ranking (different from voting - how do you like them) Real Clear Politics does an average \n",
    "- how many pos/neg tweets per candidate --- compare against number of tweets, so could say ok trump has most tweets, but not all positive - compare 'who's winning on twitter'with poll data \n",
    "- can conclude candidate more popular on twitter than in polls, alt. is that twitter is good predictor for poll data \n",
    "- what are common topics ... topic clustering - does it vary by candidate? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# First pass at exploring Twitter scraping\n",
    "- used tweepy to scrape the data & output json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "import tweetExplore_fns as fns\n",
    "import importlib\n",
    "importlib.reload(fns);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## organizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N coordinates: 10 N Tweets 10664\n"
     ]
    }
   ],
   "source": [
    "#  import data \n",
    "# with open('raw_tweets10min.json') as data_file:\n",
    "with open('../data/raw_tweetsfilter10min_poli.json') as data_file:\n",
    "# with open('raw_tweets10min_loc.json') as data_file:\n",
    "# with open('raw_tweets40min_bosSC.json') as data_file:\n",
    "    rawdata = json.load(data_file)\n",
    "\n",
    "# selects only tweets that have text \n",
    "tweet_list = fns.extract_tweets(rawdata)\n",
    "        \n",
    "#  extract indices for tweets that also have a fields of interest\n",
    "coord_index_list = fns.extract_indices(tweet_list,\"coordinates\")\n",
    "\n",
    "# extract tweets that also have a gps coordinate \n",
    "coord_tweet_list = fns.extract_tweets_fieldOfInterest(tweet_list,\"coordinates\")\n",
    "\n",
    "print('N coordinates:',len(coord_index_list), 'N Tweets', len(tweet_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RT @AnnCoulter: Isn't Pope's attack on Trump, a presidential candidate, a violation of tax-free status? Tax the Catholic Church!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list[1][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out the textblob sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump   # tweets: 1 avg polarity:\n",
      "bush   # tweets: 0 avg polarity:\n",
      "clinton   # tweets: 0 avg polarity:\n",
      "sanders   # tweets: 0 avg polarity:\n",
      "cruz   # tweets: 0 avg polarity:\n",
      "kasich   # tweets: 0 avg polarity:\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "candidates = ['trump','bush','clinton','sanders','cruz','kasich'] #track list\n",
    "# for each candidate\n",
    "for person,val in enumerate(candidates):\n",
    "    person_sentiment_polarity = []\n",
    "    person_tweetInd = []\n",
    "    person_sentiment_subjectivity = []\n",
    "    cur_person = candidates[person]\n",
    "    # for each tweet\n",
    "    for tweet,val in enumerate(tweet_list[:1]):\n",
    "        curText = tweet_list[tweet]['text']\n",
    "        cur_blob = TextBlob(curText)\n",
    "        # check that the tweet contains mention of candidate\n",
    "        if cur_blob.words.count(cur_person):\n",
    "            cur_sentiment = cur_blob.sentiment\n",
    "            # save sentiment polarity and subjectivity\n",
    "            person_sentiment_polarity.append(cur_sentiment[0])\n",
    "            person_sentiment_subjectivity.append(cur_sentiment[1])\n",
    "            # save tweet index\n",
    "            person_tweetInd.append(tweet)\n",
    "        else:\n",
    "            x = 1\n",
    "    print(candidates[person], '  # tweets:',len(person_tweetInd), 'avg polarity:', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go get this win Mr Trump  https://t.co/rYUxE2VmVg\n",
      "0.8\n",
      "[0.8]\n"
     ]
    }
   ],
   "source": [
    "print(cur_blob)\n",
    "cur_sentiment = cur_blob.sentiment\n",
    "polarity = cur_sentiment[0]\n",
    "print(polarity)\n",
    "type(polarity)\n",
    "temp_list = []\n",
    "temp_list.append(polarity)\n",
    "print(temp_list)\n",
    "# print(testl)\n",
    "# print(person_sentiment_polarity.append(testl))\n",
    "# type(cur_sentiment[0])\n",
    "# print(cur_sentiment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweetBlob = TextBlob(joinedTexts)\n",
    "candidates = ['Trump','Clinton','Kasich','Sanders','Bush','Cruz','Rubio']\n",
    "storecount = [];\n",
    "for i in candidates:\n",
    "#     print(i)\n",
    "    storecount.append(test.words.count(i))\n",
    "fig = plt.gcf()\n",
    "matplotlib.rc('xtick',labelsize = 25)\n",
    "matplotlib.rc('ytick',labelsize = 20)\n",
    "# matplotlib.rc('ylabel',labelsize = 20)\n",
    "plt.ylabel('Number of Tweets')\n",
    "ind = np.arange(len(candidates))\n",
    "rect1 = plt.bar(ind, storecount, .3, color = 'blue')\n",
    "plt.xticks(ind+.15, candidates);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot geographical coordinates of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This just lets the output of the following code samples\n",
    "#  display inline on this page, at an appropriate size.\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = (10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# setup Lambert Conformal basemap.\n",
    "m = Basemap(projection='lcc',width=12000000,height=9000000,\n",
    "            resolution='l',lat_1=45.,lat_2=55,lat_0=50,lon_0=-107.)\n",
    "#draw boundary, fill ocean color\n",
    "m.drawmapboundary(fill_color='#CCFFFF')\n",
    "# fill continents, set lake color\n",
    "m.fillcontinents(color='#66CC99',lake_color='#CCFFFF') # '#6699CC'\n",
    "# draw parallels and meridians. label only on edges of map\n",
    "parallels = np.arange(0.,81,10.)\n",
    "m.drawparallels(parallels,labels=[False,True,True,False])\n",
    "meridians = np.arange(10.,351.,20.)\n",
    "m.drawmeridians(meridians,labels=[True,False,False,True])\n",
    "# drawstates, if no linewidth will be empty\n",
    "m.drawstates(linewidth = .5)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "# m.drawcounties(linewidth = 0.05, linestyle = 'solid', color = 'k', antialiased = 1)\n",
    "\n",
    "i = 0\n",
    "while i < len(coords):\n",
    "    # get current coordinates \n",
    "    lon, lat = coords[i]\n",
    "    # convert to map projection coords.\n",
    "    # Note that lon,lat can be scalars, lists or numpy arrays.\n",
    "    xpt,ypt = m(lon,lat)\n",
    "    # convert back to lat/lon\n",
    "    lonpt, latpt = m(xpt,ypt,inverse=True)\n",
    "    m.plot(xpt,ypt,'ro')  # plot a blue dot there\n",
    "    label = 'Hello'\n",
    "    plt.Text(xpt+1000, ypt+5000, label)\n",
    "    # put some text next to the dot, offset a little bit\n",
    "    # (the offset is in map projection coordinates)\n",
    "#     plt.text(xpt+100000,ypt+100000,'Boulder (%5.1fW,%3.1fN)' % (lonpt,latpt))\n",
    "#     plt.text(xpt+100000,ypt+100000,'(%5.1fW,%3.1fN)' % (i,lonpt,latpt))\n",
    "    i = i + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (20,20)\n",
    "\n",
    " \n",
    "# make sure the value of resolution is a lowercase L,\n",
    "#  for 'low', not a numeral 1\n",
    "# my_map = Basemap(projection='ortho', lat_0=50, lon_0=-100,\n",
    "#               resolution='l', area_thresh=1000.0)\n",
    "# my_map = Basemap(projection='robin', lat_0=50, lon_0=-100,\n",
    "#               resolution='l', area_thresh=1000.0)\n",
    "my_map = Basemap(projection='merc', lat_0=42.36, lon_0=-71.06,\n",
    "              resolution='h', area_thresh=.01, \n",
    "              llcrnrlon=-71.3, llcrnrlat=42.1,\n",
    "              urcrnrlon=-70.9, urcrnrlat=42.6) \n",
    "#               llcrnrlon=-71.098709, llcrnrlat=42.328093,\n",
    "#               urcrnrlon=-71.012192, urcrnrlat=42.389994) \n",
    "my_map.drawcoastlines()\n",
    "my_map.drawcountries()\n",
    "my_map.fillcontinents(color='#66CC99')\n",
    "my_map.drawstates(linewidth = .5)\n",
    "my_map.drawmapboundary()\n",
    "my_map.drawmeridians(np.arange(0, 360, 30))\n",
    "my_map.drawparallels(np.arange(-90, 90, 30))\n",
    "# my_map.drawcounties(linewidth = 0.5)\n",
    "\n",
    "# lon = -135.3318\n",
    "# lat = 57.0799\n",
    "# x,y = my_map(lon, lat)\n",
    "# my_map.plot(x, y, 'bo', markersize=12)\n",
    "i = 0\n",
    "while i < len(coords):\n",
    "    # get current coordinates \n",
    "    lon, lat = coords[i]\n",
    "    # convert to map projection coords.\n",
    "    # Note that lon,lat can be scalars, lists or numpy arrays.\n",
    "    xpt,ypt = my_map(lon,lat)\n",
    "    # convert back to lat/lon\n",
    "    lonpt, latpt = my_map(xpt,ypt,inverse=True)\n",
    "    my_map.plot(xpt,ypt,'ro')  # plot a blue dot there\n",
    "    label = 'Hello'\n",
    "    plt.Text(xpt+1000, ypt+5000, label)\n",
    "    # put some text next to the dot, offset a little bit\n",
    "    # (the offset is in map projection coordinates)\n",
    "#     plt.text(xpt+100000,ypt+100000,'Boulder (%5.1fW,%3.1fN)' % (lonpt,latpt))\n",
    "#     plt.text(xpt+100000,ypt+100000,'(%5.1fW,%3.1fN)' % (i,lonpt,latpt))\n",
    "    i = i + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display wordcloud for tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# no_urls_no_tags = \" \".join([word for word in words.split()\n",
    "#                             if 'http' not in word\n",
    "#                                 and not word.startswith('@')\n",
    "#                                 and word != 'RT'\n",
    "#                             ])\n",
    "\n",
    "# join tweets into a single string\n",
    "joinedTexts = \" \".join(str(x) for x in texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "newSTOP = STOPWORDS\n",
    "# add stopwords to exclude from wordcloud - \n",
    "newSTOP.update(('co','https','t','RT','R','T','-RT','RT@'))\n",
    "\n",
    "#generate wordcloud\n",
    "wordcloud = WordCloud(font_path='/anaconda/envs/eq_env/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/cmmi10.ttf',\n",
    "                          stopwords=newSTOP,\n",
    "                          background_color='white',\n",
    "                          width=1700,\n",
    "                          height=1400\n",
    "                         ).generate(joinedTexts)\n",
    "# display wordcloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out the document clustering.... http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "# import mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#not super pythonic, no, not at all.\n",
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in texts:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print( 'there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(texts) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
